import numpy as np
from abc import ABC, abstractmethod

# Create an ABC for kernel functions. This is useful for creating a common interface for all kernel functions.

class Kernel(ABC):
    @abstractmethod
    def __call__(self, x, y) -> float:
        """The kernel function, K(x,y) is implemented here.



        Args:
            x : first input
            y : second input

        Returns:
            float : value of the kernel function at x and y
        """

        pass

    @abstractmethod
    def __str__(self):
        pass

    @abstractmethod
    def feature_map(x, n=1):
        """ Return a feature map representation of the input x.

        Second argument n is to specify dimension of the feature map if it is infinite dimensional.
        If the feature map is always finite dimensional, n is ignored.
        """
        pass

    @abstractmethod
    def matrix(self, X):
        n = len(X)
        K = np.zeros((n,n))
        for i in range(n):
            for j in range(n):
                K[i,j] = self(X[i], X[j])
        return K

    @abstractmethod
    def gradientX(self, x, y):
        pass

    def gradientY(self, x, y):
        """Evaluate the gradient of the kernel function with respect to the
            second argument.

            As kernel functions are symmetric, this is the same as the gradient
            with respect to the first argument. It is provided for completeness.

        Args:
            x (numeric): First argument
            y (numeric): Second argument

        Returns:
            gradient: numeric
        """
        return self.gradientX(y, x)

    @abstractmethod
    def multiDerivative(self, x, y, alpha:list[int]):
        # Compute the alpha-partial derivative of the kernel function w.r.t x.

        # Use JAX here to implement this function, or do it analytically
        raise NotImplementedError("multiDerivative not implemented")

class Linear(Kernel):
    def __call__(self, x, y):
        return np.dot(x, y)

    def __str__(self):
        return 'Linear'

    def gradientX(self, x, y):
        return y

    def gradientY(self, x, y):
        return x

    def matrix(self, X):
        return np.dot(X, X.T)
    def multiDerivative(self, x, y, alpha: list[int]):

        raise NotImplementedError("multiDerivative not implemented")


class Polynomial(Kernel):
    def __init__(self, degree):
        self.degree = degree

    def __call__(self, x, y):
        return (np.dot(x, y) + 1) ** self.degree

    def __str__(self):
        return f'Polynomial degree {self.degree}'

    def feature_map(x, n=1):
        raise NotImplementedError("feature_map not implemented")

    # Check the rest of this class - was generated by Copilot
    def gradientX(self, x, y):
        return self.degree * (np.dot(x, y) + 1) ** (self.degree - 1) * y

    def gradientY(self, x, y):
        return self.degree * (np.dot(x, y) + 1) ** (self.degree - 1) * x

    def matrix(self, X):
        return (np.dot(X, X.T) + 1) ** self.degree

    def multiDerivative(self, x, y, alpha: list[int]):

        raise NotImplementedError("multiDerivative not implemented")


class Gaussian(Kernel):
    def __init__(self, sigma):
        self.sigma = sigma

    def __call__(self, x, y):
        return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * self.sigma ** 2))

    def __str__(self):
        return f'Gaussian sigma {self.sigma}'

    def feature_map(x, n=1):
        raise NotImplementedError("feature_map not implemented")

    def gradientX(self, x, y):
        factor = -1 / self.sigma ** 2
        return factor * (x - y) * self.__call__(x, y)

    def matrix(self, X):
        n = len(X)
        K = np.zeros((n, n))
        for i in range(n):
            for j in range(i, n):
                K[i, j] = self.__call__(X[i], X[j])
                K[j, i] = K[i, j]
        return K

    def multiDerivative(self, x, y, alpha: list[int]):
        Kxy = self.__call__(x, y)
        if len(alpha) == 1 and alpha[0] == 1:
            return self.gradientX(x, y)
        elif len(alpha) == 2 and all(a == 1 for a in alpha):
            factor = 1 / self.sigma ** 4
            term1 = self.sigma ** 2 - np.sum((x - y) ** 2)
            return factor * (x - y) ** 2 * Kxy + term1 * Kxy / self.sigma ** 2
        else:
            raise NotImplementedError("Higher order derivatives are not implemented")


class Exponential(Kernel):
    def __init__(self, l):
        self.l = l

    # Class implementation by Copilot - check if it is correct
    def __call__(self, x, y):
        return np.exp(-np.linalg.norm(x - y) / self.l)

    def __str__(self):
        return f'Exponential l {self.l}'

    def feature_map(x, n=1):
        raise NotImplementedError("feature_map not implemented")

    def gradientX(self, x, y):
        return -self(x, y) * (x - y) / self.l ** 2

    def matrix(self, X):
        n = len(X)
        K = np.zeros((n,n))
        for i in range(n):
            for j in range(i, n):
                K[i,j] = self(X[i], X[j])
                K[j,i] = K[i,j]
        return K

    def multiDerivative(self, x, y, alpha: list[int]):

        raise NotImplementedError("multiDerivative not implemented")

class featureMapKernel(Kernel):
    # A kernel can be defined by a feature map and a set of weights.
    # Use this to define a kernel function.

    # Implementation by Copilot - check if it is correct
    def __init__(self, feature_map, weights, alpha_deriv_formula):
        self.feature_map = feature_map
        self.weights = weights
        self.alpha_deriv_formula = alpha_deriv_formula

    def __call__(self, x, y):
        return np.dot(self.feature_map(x), self.feature_map(y))

    def __str__(self):
        return 'Feature Map Kernel'

    def gradientX(self, x, y):
        return self.feature_map(y)

    def matrix(self, X):
        n = len(X)
        K = np.zeros((n,n))
        for i in range(n):
            for j in range(i, n):
                K[i,j] = self(X[i], X[j])
                K[j,i] = K[i,j]
        return K

    def multiDerivative(self, x, y, alpha: list[int]):
        # Use alpha_deriv_formula to compute the alpha-partial derivative of the kernel function w.r.t x.

        raise NotImplementedError("multiDerivative not implemented")