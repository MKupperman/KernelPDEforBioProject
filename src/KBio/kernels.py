import numpy as np
from abc import ABC, abstractmethod

# Create an ABC for kernel functions. This is useful for creating a common interface for all kernel functions.

class Kernel(ABC):
    @abstractmethod
    def __call__(self, x, y) -> float:
        """The kernel function, K(x,y) is implemented here.



        Args:
            x : first input
            y : second input

        Returns:
            float : value of the kernel function at x and y
        """

        pass

    @abstractmethod
    def __str__(self):
        pass

    @abstractmethod
    def matrix(self, X):
        n = len(X)
        K = np.zeros((n,n))
        for i in range(n):
            for j in range(n):
                K[i,j] = self(X[i], X[j])
        return K

    @abstractmethod
    def gradientX(self, x, y):
        pass

    def gradientY(self, x, y):
        """Evaluate the gradient of the kernel function with respect to the
            second argument.

            As kernel functions are symmetric, this is the same as the gradient
            with respect to the first argument. It is provided for completeness.

        Args:
            x (numeric): First argument
            y (numeric): Second argument

        Returns:
            gradient: numeric
        """
        return self.gradientX(y, x)

    @abstractmethod
    def multiDerivative(self, x, y, alpha:list[int]):
        # Use JAX here to implement this function, or do it analytically
        raise NotImplementedError("multiDerivative not implemented")

class Linear(Kernel):
    def __call__(self, x, y):
        return np.dot(x, y)

    def __str__(self):
        return 'Linear'

    def gradientX(self, x, y):
        return y

    def gradientY(self, x, y):
        return x

    def matrix(self, X):
        return np.dot(X, X.T)

class Polynomial(Kernel):
    def __init__(self, degree):
        self.degree = degree

    def __call__(self, x, y):
        return (np.dot(x, y) + 1) ** self.degree

    def __str__(self):
        return f'Polynomial degree {self.degree}'

    # Check the rest of this class - was generated by Copilot
    def gradientX(self, x, y):
        return self.degree * (np.dot(x, y) + 1) ** (self.degree - 1) * y

    def gradientY(self, x, y):
        return self.degree * (np.dot(x, y) + 1) ** (self.degree - 1) * x

    def matrix(self, X):
        return (np.dot(X, X.T) + 1) ** self.degree

class Gaussian(Kernel):
    def __init__(self, sigma):
        self.sigma = sigma

    def __call__(self, x, y):
        return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * self.sigma ** 2))

    def __str__(self):
        return f'Gaussian sigma {self.sigma}'

    def gradientX(self, x, y):
        return -self(x, y) * (x - y) / self.sigma ** 2

    def matrix(self, X):
        n = len(X)
        # Find a way to vectorize this - this is a naive implementation
        K = np.zeros((n,n))
        for i in range(n):
            for j in range(i, n):
                K[i,j] = self(X[i], X[j])
                K[j,i] = K[i,j]
        return K

class Exponential(Kernel):
    def __init__(self, l):
        self.l = l

    # Class implementation by Copilot - check if it is correct
    def __call__(self, x, y):
        return np.exp(-np.linalg.norm(x - y) / self.l)

    def __str__(self):
        return f'Exponential l {self.l}'

    def gradientX(self, x, y):
        return -self(x, y) * (x - y) / self.l ** 2

    def matrix(self, X):
        n = len(X)
        K = np.zeros((n,n))
        for i in range(n):
            for j in range(i, n):
                K[i,j] = self(X[i], X[j])
                K[j,i] = K[i,j]
        return K
